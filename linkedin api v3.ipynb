{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Desarrollador Backend\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "start = 0\n",
    "jobs_per_page = 25\n",
    "total_jobs = 25\n",
    "data = []\n",
    "\n",
    "while start < total_jobs:\n",
    "\n",
    "    url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Desarrollador%20Backend&location=Lima%2C%20Per%C3%BA&f_TPR=r2592000&geoId=100829422&trk=public_jobs_jobs-search-bar_search-submit&refresh=true&start={start}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    jobs = soup.find_all('li')\n",
    "\n",
    "    for job in jobs:\n",
    "        title = job.find('h3', class_='base-search-card__title').text.strip()\n",
    "        url_element = job.find('a', class_='base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]')\n",
    "        if url_element is not None:\n",
    "            job_url = url_element['href']\n",
    "        else:\n",
    "            url_element = job.find('a', class_='base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card')\n",
    "            if url_element is not None:\n",
    "                job_url = url_element['href']\n",
    "            else:\n",
    "                job_url = \"No se encontró URL\"\n",
    "\n",
    "        location = job.find('span', class_='job-search-card__location').text.strip()\n",
    "\n",
    "        publishdate_element = job.find('time',class_='job-search-card__listdate')\n",
    "\n",
    "        if publishdate_element is not None:\n",
    "            publishdate = publishdate_element.text.strip()\n",
    "        else:\n",
    "            publishdate_element = job.find('time',class_='job-search-card__listdate--new')\n",
    "            if publishdate_element is not None:\n",
    "                publishdate = publishdate_element.text.strip()\n",
    "            else:\n",
    "                publishdate = \"No se encontró fecha de publicación\"\n",
    "        \n",
    "        jobcompany = job.find('h4',class_='base-search-card__subtitle').text.strip()\n",
    "\n",
    "        job_response = requests.get(job_url)\n",
    "        job_soup = BeautifulSoup(job_response.text, 'html.parser')\n",
    "\n",
    "        description_boxes = job_soup.find_all('div',class_='show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden')\n",
    "\n",
    "        if len(description_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        ul_texts = []\n",
    "\n",
    "        for box in description_boxes:\n",
    "            string_tags = list(box.strings)\n",
    "            \n",
    "            string_texts = [p.text.strip() for p in string_tags if p.text.strip() != '']\n",
    "\n",
    "            ul_texts.extend(string_texts)\n",
    "\n",
    "        data.append([title, job_url,location,publishdate,jobcompany, \"\\n\".join(ul_texts)])\n",
    "\n",
    "    start += jobs_per_page\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Jobname', 'URL', 'Location', 'Date', 'Company', 'Description'])\n",
    "\n",
    "df.to_csv('Backendjobs.csv', sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Desarrollador Frontend\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "start = 0\n",
    "jobs_per_page = 25\n",
    "total_jobs = 25\n",
    "data = []\n",
    "\n",
    "while start < total_jobs:\n",
    "\n",
    "    url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Desarrollador%20Frontend&location=Lima%2C%20Per%C3%BA&f_TPR=r2592000&geoId=100829422&trk=public_jobs_jobs-search-bar_search-submit&refresh=true&start={start}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    jobs = soup.find_all('li')\n",
    "\n",
    "    for job in jobs:\n",
    "        title = job.find('h3', class_='base-search-card__title').text.strip()\n",
    "        url_element = job.find('a', class_='base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]')\n",
    "        if url_element is not None:\n",
    "            job_url = url_element['href']\n",
    "        else:\n",
    "            url_element = job.find('a', class_='base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card')\n",
    "            if url_element is not None:\n",
    "                job_url = url_element['href']\n",
    "            else:\n",
    "                job_url = \"No se encontró URL\"\n",
    "        \n",
    "        location = job.find('span', class_='job-search-card__location').text.strip()\n",
    "\n",
    "        publishdate_element = job.find('time',class_='job-search-card__listdate')\n",
    "\n",
    "        if publishdate_element is not None:\n",
    "            publishdate = publishdate_element.text.strip()\n",
    "        else:\n",
    "            publishdate_element = job.find('time',class_='job-search-card__listdate--new')\n",
    "            if publishdate_element is not None:\n",
    "                publishdate = publishdate_element.text.strip()\n",
    "            else:\n",
    "                publishdate = \"No se encontró fecha de publicación\"\n",
    "        \n",
    "        jobcompany = job.find('h4',class_='base-search-card__subtitle').text.strip()\n",
    "\n",
    "        job_response = requests.get(job_url)\n",
    "        job_soup = BeautifulSoup(job_response.text, 'html.parser')\n",
    "        \n",
    "        description_boxes = job_soup.find_all('div',class_='show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden')\n",
    "\n",
    "        if len(description_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        ul_texts = []\n",
    "\n",
    "        for box in description_boxes:\n",
    "            string_tags = list(box.strings)\n",
    "            \n",
    "            string_texts = [p.text.strip() for p in string_tags if p.text.strip() != '']\n",
    "\n",
    "            ul_texts.extend(string_texts)\n",
    "\n",
    "        data.append([title, job_url,location,publishdate,jobcompany, \"\\n\".join(ul_texts)])\n",
    "\n",
    "    start += jobs_per_page\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Jobname', 'URL', 'Location', 'Date', 'Company', 'Description'])\n",
    "\n",
    "df.to_csv('Frontendjobs.csv', sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Desarrollador Fullstack\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "start = 0\n",
    "jobs_per_page = 25\n",
    "total_jobs = 25\n",
    "data = []\n",
    "\n",
    "while start < total_jobs:\n",
    "\n",
    "    url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Desarrollador%20Fullstack&location=Lima%2C%20Per%C3%BA&f_TPR=r2592000&geoId=100829422&trk=public_jobs_jobs-search-bar_search-submit&refresh=true&start={start}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    jobs = soup.find_all('li')\n",
    "\n",
    "    for job in jobs:\n",
    "        title = job.find('h3', class_='base-search-card__title').text.strip()\n",
    "        url_element = job.find('a', class_='base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]')\n",
    "        if url_element is not None:\n",
    "            job_url = url_element['href']\n",
    "        else:\n",
    "            url_element = job.find('a', class_='base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card')\n",
    "            if url_element is not None:\n",
    "                job_url = url_element['href']\n",
    "            else:\n",
    "                job_url = \"No se encontró URL\"\n",
    "\n",
    "        location = job.find('span', class_='job-search-card__location').text.strip()\n",
    "\n",
    "        publishdate_element = job.find('time',class_='job-search-card__listdate')\n",
    "\n",
    "        if publishdate_element is not None:\n",
    "            publishdate = publishdate_element.text.strip()\n",
    "        else:\n",
    "            publishdate_element = job.find('time',class_='job-search-card__listdate--new')\n",
    "            if publishdate_element is not None:\n",
    "                publishdate = publishdate_element.text.strip()\n",
    "            else:\n",
    "                publishdate = \"No se encontró fecha de publicación\"\n",
    "        \n",
    "        jobcompany = job.find('h4',class_='base-search-card__subtitle').text.strip()\n",
    "\n",
    "        job_response = requests.get(job_url)\n",
    "        job_soup = BeautifulSoup(job_response.text, 'html.parser')\n",
    "        \n",
    "        description_boxes = job_soup.find_all('div',class_='show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden')\n",
    "\n",
    "        if len(description_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        ul_texts = []\n",
    "\n",
    "        for box in description_boxes:\n",
    "            string_tags = list(box.strings)\n",
    "            \n",
    "            string_texts = [p.text.strip() for p in string_tags if p.text.strip() != '']\n",
    "\n",
    "            ul_texts.extend(string_texts)\n",
    "\n",
    "        data.append([title, job_url,location,publishdate,jobcompany, \"\\n\".join(ul_texts)])\n",
    "\n",
    "    start += jobs_per_page\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Jobname', 'URL', 'Location', 'Date', 'Company', 'Description'])\n",
    "\n",
    "df.to_csv('Fullstackjobs.csv', sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Desarrollador Móvil\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "start = 0\n",
    "jobs_per_page = 25\n",
    "total_jobs = 25\n",
    "data = []\n",
    "\n",
    "while start < total_jobs:\n",
    "\n",
    "    url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Desarrollador%20M%C3%B3vil&location=Lima%2C%20Per%C3%BA&f_TPR=r2592000&geoId=100829422&trk=public_jobs_jobs-search-bar_search-submit&refresh=true&start={start}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    jobs = soup.find_all('li')\n",
    "\n",
    "    for job in jobs:\n",
    "        title = job.find('h3', class_='base-search-card__title').text.strip()\n",
    "        url_element = job.find('a', class_='base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]')\n",
    "        if url_element is not None:\n",
    "            job_url = url_element['href']\n",
    "        else:\n",
    "            url_element = job.find('a', class_='base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card')\n",
    "            if url_element is not None:\n",
    "                job_url = url_element['href']\n",
    "            else:\n",
    "                job_url = \"No se encontró URL\"\n",
    "\n",
    "        location = job.find('span', class_='job-search-card__location').text.strip()\n",
    "\n",
    "        publishdate_element = job.find('time',class_='job-search-card__listdate')\n",
    "\n",
    "        if publishdate_element is not None:\n",
    "            publishdate = publishdate_element.text.strip()\n",
    "        else:\n",
    "            publishdate_element = job.find('time',class_='job-search-card__listdate--new')\n",
    "            if publishdate_element is not None:\n",
    "                publishdate = publishdate_element.text.strip()\n",
    "            else:\n",
    "                publishdate = \"No se encontró fecha de publicación\"\n",
    "        \n",
    "        jobcompany = job.find('h4',class_='base-search-card__subtitle').text.strip()\n",
    "\n",
    "        job_response = requests.get(job_url)\n",
    "        job_soup = BeautifulSoup(job_response.text, 'html.parser')\n",
    "        \n",
    "        description_boxes = job_soup.find_all('div',class_='show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden')\n",
    "\n",
    "        if len(description_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        ul_texts = []\n",
    "\n",
    "        for box in description_boxes:\n",
    "            string_tags = list(box.strings)\n",
    "            \n",
    "            string_texts = [p.text.strip() for p in string_tags if p.text.strip() != '']\n",
    "\n",
    "            ul_texts.extend(string_texts)\n",
    "\n",
    "        data.append([title, job_url,location,publishdate,jobcompany, \"\\n\".join(ul_texts)])\n",
    "\n",
    "    start += jobs_per_page\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Jobname', 'URL', 'Location', 'Date', 'Company', 'Description'])\n",
    "\n",
    "df.to_csv('Moviljobs.csv', sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingenieria de datos\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "start = 0\n",
    "jobs_per_page = 25\n",
    "total_jobs = 25\n",
    "data = []\n",
    "\n",
    "while start < total_jobs:\n",
    "\n",
    "    url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Ingenier%C3%ADa%20de%20datos&location=Lima%2C%20Per%C3%BA&f_TPR=r2592000&geoId=100829422&trk=public_jobs_jobs-search-bar_search-submit&refresh=true&start={start}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    jobs = soup.find_all('li')\n",
    "\n",
    "    for job in jobs:\n",
    "        title = job.find('h3', class_='base-search-card__title').text.strip()\n",
    "        url_element = job.find('a', class_='base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]')\n",
    "        if url_element is not None:\n",
    "            job_url = url_element['href']\n",
    "        else:\n",
    "            url_element = job.find('a', class_='base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card')\n",
    "            if url_element is not None:\n",
    "                job_url = url_element['href']\n",
    "            else:\n",
    "                job_url = \"No se encontró URL\"\n",
    "\n",
    "        location = job.find('span', class_='job-search-card__location').text.strip()\n",
    "\n",
    "        publishdate_element = job.find('time',class_='job-search-card__listdate')\n",
    "\n",
    "        if publishdate_element is not None:\n",
    "            publishdate = publishdate_element.text.strip()\n",
    "        else:\n",
    "            publishdate_element = job.find('time',class_='job-search-card__listdate--new')\n",
    "            if publishdate_element is not None:\n",
    "                publishdate = publishdate_element.text.strip()\n",
    "            else:\n",
    "                publishdate = \"No se encontró fecha de publicación\"\n",
    "        \n",
    "        jobcompany = job.find('h4',class_='base-search-card__subtitle').text.strip()\n",
    "\n",
    "        job_response = requests.get(job_url)\n",
    "        job_soup = BeautifulSoup(job_response.text, 'html.parser')\n",
    "        \n",
    "        description_boxes = job_soup.find_all('div',class_='show-more-less-html__markup show-more-less-html__markup--clamp-after-5 relative overflow-hidden')\n",
    "\n",
    "        if len(description_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        ul_texts = []\n",
    "\n",
    "        for box in description_boxes:\n",
    "            string_tags = list(box.strings)\n",
    "            \n",
    "            string_texts = [p.text.strip() for p in string_tags if p.text.strip() != '']\n",
    "\n",
    "            ul_texts.extend(string_texts)\n",
    "\n",
    "        data.append([title, job_url,location,publishdate,jobcompany, \"\\n\".join(ul_texts)])\n",
    "\n",
    "    start += jobs_per_page\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Jobname', 'URL', 'Location', 'Date', 'Company', 'Description'])\n",
    "\n",
    "df.to_csv('Datosjobs.csv', sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivos_csv = ['Backendjobs.csv', 'Datosjobs.csv', 'Frontendjobs.csv', 'Fullstackjobs.csv', 'Moviljobs.csv']\n",
    "\n",
    "# Crear una lista para almacenar los DataFrames de cada archivo CSV\n",
    "dataframes = []\n",
    "\n",
    "# Leer cada archivo CSV y almacenarlo en la lista de DataFrames\n",
    "for archivo in archivos_csv:\n",
    "    df = pd.read_csv(archivo, sep='\\t')\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Unir los DataFrames en uno solo\n",
    "df_unido = pd.concat(dataframes)\n",
    "\n",
    "# Agregar una nueva columna 'Jobid' con incremento para cada fila\n",
    "df_unido.insert(0, 'Jobid', range(1, len(df_unido) + 1))\n",
    "\n",
    "# Guardar el DataFrame unido en un nuevo archivo CSV\n",
    "df_unido.to_csv('jobs.csv', sep='\\t', index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Jobid                                            Jobname  \\\n",
      "0      1                       FullStack Developer (Remote)   \n",
      "1      2                               Full-Stack Developer   \n",
      "2      3  Programador / Desarrollador Fullstack Junior -...   \n",
      "3      4                             Programador Full Stack   \n",
      "4      5                                  Backend Developer   \n",
      "\n",
      "                                                 URL          Location  \\\n",
      "0  https://pe.linkedin.com/jobs/view/fullstack-de...  Lima, Lima, Peru   \n",
      "1  https://pe.linkedin.com/jobs/view/full-stack-d...  Lima, Lima, Peru   \n",
      "2  https://pe.linkedin.com/jobs/view/programador-...   Ate, Lima, Peru   \n",
      "3  https://pe.linkedin.com/jobs/view/programador-...  Lima, Lima, Peru   \n",
      "4  https://pe.linkedin.com/jobs/view/backend-deve...        Lima, Peru   \n",
      "\n",
      "          Date            Company  \\\n",
      "0   1 week ago               Atom   \n",
      "1   3 days ago  Rivka Development   \n",
      "2   1 week ago           TRBA SAC   \n",
      "3   1 week ago               CTIC   \n",
      "4  4 hours ago             SOAINT   \n",
      "\n",
      "                                         Description  \n",
      "0  Atom Chat\\nes una plataforma SaaS B2B de Comer...  \n",
      "1  Please submit your resume in\\nENGLISH\\n! We lo...  \n",
      "2  Somos una empresa del sector Industrial, nos e...  \n",
      "3  En CTIC nos encontramos en la búsqueda del mej...  \n",
      "4  En\\nSoaint\\npredomina la cultura del knowledge...  \n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo CSV en un DataFrame\n",
    "df = pd.read_csv('jobs.csv', sep='\\t')\n",
    "\n",
    "# Mostrar las 5 primeras filas del DataFrame\n",
    "print(df.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
